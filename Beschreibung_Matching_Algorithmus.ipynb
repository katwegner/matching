{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Algorithm basierend auf Interessen und/oder Profiltexten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nayjN8Vrodod",
    "outputId": "a07fee49-de2d-4166-9ff5-28eb70e9e537"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text_preprocessing(df):\n",
    "    # removing special characters and stop words from the text\n",
    "    stop_words_l=stopwords.words('english')\n",
    "    df['profiles_cleaned']=df.profiles.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )\n",
    "    # local importance: tf-idf\n",
    "    tfidfvectoriser=TfidfVectorizer(max_features=64)\n",
    "    tfidfvectoriser.fit(df.profiles_cleaned)\n",
    "    tfidf_vectors=tfidfvectoriser.transform(df.profiles_cleaned)\n",
    "    tfidf_vectors=tfidf_vectors.toarray()\n",
    "    # tokenize and pad every document to make them of the same size\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(df.profiles_cleaned)\n",
    "    tokenized_documents=tokenizer.texts_to_sequences(df.profiles_cleaned)\n",
    "    tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')\n",
    "    vocab_size=len(tokenizer.word_index)+1\n",
    "    return df, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size\n",
    "\n",
    "def most_similar(df, id,similarity_matrix):\n",
    "    print (f'Profile: {df.iloc[id][\"names\"]}')\n",
    "    print ('\\n')\n",
    "    print (f'Similar Profiles using Cosine Similarity')\n",
    "    similar_ix=np.argsort(similarity_matrix[id])[::-1]\n",
    "    for ix in similar_ix:\n",
    "        if ix==id:\n",
    "            continue\n",
    "        print('\\n')\n",
    "        print (f'Profile: {df.iloc[ix][\"names\"]}')\n",
    "        print (f'Cosine Similarity: {similarity_matrix[id][ix]}')\n",
    "\n",
    "def get_glove_similarity(df, df_idx, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size):\n",
    "    # reading Glove word embeddings into a dictionary with \"word\" as key and values as word vectors\n",
    "    embeddings_index = dict()\n",
    "    with open('glove.6B.100d.txt') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    # creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. \n",
    "    embedding_matrix=np.zeros((vocab_size,100))\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # tf-idf vectors do not keep the original sequence of words, converting them into actual word sequences from the documents\n",
    "    document_embeddings=np.zeros((len(tokenized_paded_documents),100))\n",
    "    words=tfidfvectoriser.get_feature_names_out()\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(len(words)):\n",
    "            document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]     \n",
    "    document_embeddings=document_embeddings/np.sum(tfidf_vectors,axis=1).reshape(-1,1)\n",
    "    most_similar(df, df_idx,cosine_similarity(document_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gegeben sind verschiedene Profile:\n",
    "\n",
    "![Example Profiles](profiles.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wv6pYEhdodoi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            profiles           names\n",
      "0                        dog standup paddling hiking    outdoor_anna\n",
      "1            fortnite boardgame tetris poker netflix    gamer_thomas\n",
      "2  paragliding kite surfing aethletics skiing mou...  adventure_alex\n",
      "3                                guitar singer drums      rock_robin\n"
     ]
    }
   ],
   "source": [
    "# Create profiles as dataframe\n",
    "outdoor_anna = [\"dog standup paddling hiking\"]\n",
    "gamer_thomas = [\"fortnite boardgame tetris poker netflix\"]\n",
    "adventure_alex = [\"paragliding kite surfing aethletics skiing mountain sports\"]\n",
    "rock_robin = [\"guitar singer drums\"]\n",
    "profiles = [outdoor_anna, gamer_thomas, adventure_alex, rock_robin]\n",
    "df=pd.DataFrame(profiles,columns=['profiles'])\n",
    "df[\"names\"] = [\"outdoor_anna\", \"gamer_thomas\", \"adventure_alex\", \"rock_robin\"]\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Für diese Profile soll ein Matching-Score errechnet werden. \n",
    "Zwei Profile haben eine hohe Matching-Score, wenn die Interessen der Personen verwandt sind. Interessen sind verwandt, wenn beide Interessen häufig im selben globalen Kontext genannt werden (zum Beispiel Gitarre spielen wird häufig im selben Kontext genannt wie singen). \n",
    "GloVe (Global Vector) ist ein vortrainiertes Modell, das die Häufigkeit des gemeinsamen Auftretens von Wörtern beinhaltet.  \n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "\n",
    "Die Ähnlichkeit der Interessen wird mathematisch durch die Cosinus-Ähnlichkeit errechnet. Dafür werden die Interessen zunächst in eine quantifizierbare Form gebracht. Dazu gehören:\n",
    "* Kleinschreibung und Entfernung von Sonderzeichen\n",
    "* Vektorisierung (Wörter werden einzeln betrachtet und erhalten eine Zahl)\n",
    "* Normierung durch lokale Frequenz gewichtet durch Häufigkeit pro Profil und durch Häufigkeit in allen Profilen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cgea0X8Nodot"
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "df, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size = text_preprocessing(df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7oG1uuGodpp"
   },
   "source": [
    "#### Glove embeddings\n",
    "Die Werte des vortrainierten GloVe Modells werden über die Datei glove.6B.100d.txt eingeladen ([Link hier](https://nlp.stanford.edu/projects/glove/)). Eine Matrix mit Ähnlichkeitswerten für das lokale Vokabular wird aus dem GloVe-Modell gezogen. Die Ähnlichkeitswerte werden dann gewichtet durch die lokale Häufigkeit der Wörter. Ein künstliches Beispiel für die Ähnlichkeitswerte der Wörter, die wir aus dem GloVe-Modell erhalten, könnte wie folgt aussehen:\n",
    "\n",
    "<img src=\"vector_representation2.png\" alt=\"Vector Representation\" width=\"500\"/>\n",
    "\n",
    "Jedes Wort wird als Vektor ausgedrückt, wobei die Position die semantische Ähnlichkeit zu den benachbarten Wörtern ausdrückt. \n",
    "\n",
    "Zuletztt wird für jedes Paar von Profilen die Cosinus-Ähnlichkeit berechnet nach folgender Formel:\n",
    "\n",
    "<img src=\"cosine_formula.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: outdoor_anna\n",
      "\n",
      "\n",
      "Similar Profiles using Cosine Similarity\n",
      "\n",
      "\n",
      "Profile: adventure_alex\n",
      "Cosine Similarity: 0.6357523292935934\n",
      "\n",
      "\n",
      "Profile: gamer_thomas\n",
      "Cosine Similarity: 0.26693923773546746\n",
      "\n",
      "\n",
      "Profile: rock_robin\n",
      "Cosine Similarity: 0.1822169619312551\n"
     ]
    }
   ],
   "source": [
    "get_glove_similarity(df, 0, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiltext\n",
    "Matching-Scores können ebenfalls anhand von Profiltexten berrechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Junior_DS_Caro\n",
      "\n",
      "\n",
      "Similar Profiles using Cosine Similarity\n",
      "\n",
      "\n",
      "Profile: Senior_DS_Hanna\n",
      "Cosine Similarity: 0.9222393739200262\n",
      "\n",
      "\n",
      "Profile: UX_Designer_Tanja\n",
      "Cosine Similarity: 0.8539059446766031\n"
     ]
    }
   ],
   "source": [
    "Junior_DS_Caro = [\"Junior data scientist with an MSc in Applied mathematics with experience throughout the data processing lifecycle. Programming, data wrangling/data cleaning, and analysis skills. Interested in developing machine learning applications to solve real-world problems.\"]\n",
    "Senior_DS_Hanna = [\"Passionate about data-based insights and decisions since 2016. I have 6+ years of experience in data-driven research and data consulting, and 2+ of experience in agile project management and cross-functional team work. I hold a PhD in psychology and neuroscience and further applied data science methodologies in the area of mobility behavior. I'm offering a team-oriented, proactive and research mindset. Currently, I'm expanding my skills in cloud services and big data methodologies (e.g. Apache Spark). Since I believe that the world needs more female leaders, I want to contribute to prepare young women for a career in female leadership.\"]\n",
    "UX_Designer_Tanja = [\"I'm a Munich-based UX Designer & Researcher with a B.Sc. in Applied Psychology and professional training in Acting. Acting equipped me with observational & active listening skills, whereas psychology helped me develop hard skills like critical thinking & statistical analysis, and made me understand the cognitive process behind human behavior even better.\"]\n",
    "profiles = [Junior_DS_Caro, Senior_DS_Hanna, UX_Designer_Tanja]\n",
    "df=pd.DataFrame(profiles,columns=['profiles'])\n",
    "df[\"names\"] = [\"Junior_DS_Caro\", \"Senior_DS_Hanna\", \"UX_Designer_Tanja\"]\n",
    "df, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size = text_preprocessing(df) \n",
    "get_glove_similarity(df, 0, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zufällig gewählte Interessen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: profil1\n",
      "\n",
      "\n",
      "Similar Profiles using Cosine Similarity\n",
      "\n",
      "\n",
      "Profile: profil4\n",
      "Cosine Similarity: 0.9360698700961971\n",
      "\n",
      "\n",
      "Profile: profil5\n",
      "Cosine Similarity: 0.9355171719469483\n",
      "\n",
      "\n",
      "Profile: profil3\n",
      "Cosine Similarity: 0.9109724133547961\n",
      "\n",
      "\n",
      "Profile: profil2\n",
      "Cosine Similarity: 0.9089423243865048\n"
     ]
    }
   ],
   "source": [
    "profil1 = ['Duranguense Ebel Neo Metal 8 Ball Pool Painkiller Swedish Doom Metal Chapati Tahari by ASL Ylands Smelt Mega Man II Act of War Mongolian Hip Hop Aerial Yoga Eggomania Underwater Sports Pollack Ska Argentino TOGG Pandora Poland OAMC Badminton Omaha Indie Japanese Idm Kishu Warhammer: Vermintide 2 Ark Turkish Trap Pop Malmo Indie Breakout Daniel Malchert Chinese Indie Pop Filmi Belgian Indie Rock Talentos Brasileiros Dandie Dinmont Terrier Onzie Earthsiege Peanut Hi no Tori Hououhen Gaou no Bouken Mario Kart Tour Boucheron Day Trading Tsukiuta B-Movie Lifestyle Sonic & Sega All-Stars Racing Villeroy & Boch Serbian Indie']\n",
    "profil2 = [ 'The Whispered World Dnafight Greyhound No Wave Swiss Black Metal Japanese Concert Band Shirley Temple Black Malian Traditional Dueber-Hampden Chanson Virale Tzatziki Prey the Stars Trivia Crack Korean Phantom Singer Rap Metal David Ramsay Sniper Fury German Metal The Warriors Danish Electropop Jazz Trumpet Jazz Organ Kenyan Traditional Tzadik Rap Nacional Antigo Tänzer Kinect Adventures! Japanese Ska Daniela Villegas OPI Transpop Dutch Death Metal Art Song Inov 8 Cheerleading Stupell Industries Rebecca Minkoff Uk Contemporary R&B Pittsburgh Indie Taur Tekka Maki Simutrans Vgm Instrumental Korean Underground Rap Svensk Progg Turin Indie Musica Valenciana Crypto Horror Drama Soul Blues']\n",
    "profil3 =  ['Ylati Appalachian Folk Balfolk Chicago House Bloco Croatia Exotica Grand Anglo_Français Blanc et Orange Psychology Of Collecting Cape Breton Folk The Legend of Korra Cut the Rope\\xa0Magic Streaker Afro Funk Yemeni Traditional Transport Tycoon Fuse New Ghostbusters II Frutti di scusa Modern Hard Rock Dutch Smoushond Dubstep FIFA Gbvfi Historic Value Indie Rock Extremadura Rolling Sky Wipeout in the Zone Histoire Pour Enfants Star-Trek Bridge Commander Dutch Singer Songwriter Gloria Vanderbilt Australian Hip Hop Hanes Ultimate Folklore Nuevo Argentino BYD Auto Champagne Classic Afrobeat ZEW New Tribe Etnies Perry Ellis Atlanta Metal Austrian Contemporary Classical Gaita Zuliana Car Manufactures Harrier Onex Pitfall!']\n",
    "profil4  =['Area 51 Garoto Red Dead Revolver Singaporean Punk German Punk Rock Didgeridoo Treeing Feist Peter I Island Flying Heroes J.crew Hottāman no Chitei Tanken Qed London Uk Diy Punk Pegu ModWay Russian Death Metal Musica Maringaense Electronica Argentina Gothic Symphonic Metal Sparkle 3 Genesis Coptic Hymn Saskatchewan Indie Yam Teen Drama Cane Paratore Schweizer Rap Reggae Tico Disco No. 1 City of Heroes Techno Kitten Adventure Madagascar Belarus Rock In Opposition Israeli Pop Basketball Psychedelic Trance Scottish Rock New Ghostbusters II Pop Dance Moravian Folk Mexican Pop One Person Band Burmese Western Americana Sweetcorn Apple Tolkien Metal Genesis Motor 2048 Descent']\n",
    "profil5 = [ \"Drumfunk New Wave Of Thrash Metal Jazmin Sour Kart Racing Irish Accordion Albanian Hip Hop Theremin Brussels Sprouts Realsports Tennis Oc Rap Splatoon Wolfpack Niue Illinois Watch Company Thai Teen Pop Volkstumliche Musik ASUAG Alexander Shorokhoff Belfast Metal Yemeni Pop Bulgarian Hip Hop Rock Kapak Russian Dance Pop Gose Shabad Dunker New Romantic Georgian Pop Aguaymanto Sour Lithuanian Rock Sparkle 2 Evo King Arthur's Gold Latin Ska Squish 'em Codfish Mexican Hardcore Mullet Korean Hardcore Country Solipsynthm Bellini Frame Denim Faroese Folk Eastern Atlántico Sur Enlightenment Film Gujarati Pop Pinoy Singer Songwriter Cutthroats Roller In Line Hockey\"]\n",
    "profiles = [profil1, profil2, profil3, profil4, profil5]\n",
    "df=pd.DataFrame(profiles,columns=['profiles'])\n",
    "df[\"names\"] = [\"profil1\", \"profil2\", \"profil3\", \"profil4\", \"profil5\"]\n",
    "df, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size = text_preprocessing(df) \n",
    "get_glove_similarity(df, 0, tfidf_vectors, tokenizer, tfidfvectoriser, tokenized_paded_documents, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des eigenen Corpus\n",
    "\n",
    "Vorteile:\n",
    "- Seltene Wörter wie zum Beispiel Fortnite sind besser repräsentiert\n",
    "- bigrams können trainiert werden (zum Beispiel \"Hatha Yoga\" oder \"Mountain Biking\")\n",
    "- Akkurate Repraesentation von deutschen Woertern und Ausdruecken\n",
    "\n",
    "Beschreibung des Trainings: https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quellen\n",
    "* https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630\n",
    "* https://nlp.stanford.edu/pubs/glove.pdf\n",
    "* https://github.com/stanfordnlp/GloVe\n",
    "* https://en.wikipedia.org/wiki/GloVe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "J3H7zfoTodpF",
    "L7oG1uuGodpp"
   ],
   "name": "Document Similarities.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc2481959a60afd9bf0bc5ec3d2d54a0840fb0f3b39773d13a5add663980b87d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
